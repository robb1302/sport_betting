{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from os import listdir\n",
    "os.chdir(\"c:/Users/Robert/Documents/Projekte/dev/sport_betting/\")\n",
    "import config as CONFIG\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.provide_data import get_model_data\n",
    "from src.models.evaluate import custom_classification_report, custom_lazy_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 5246\n",
      "Valid 1952\n",
      "Test 1952\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_model_data(filename = \"Train\")\n",
    "X_valid, y_valid = get_model_data(filename = \"Valid\")\n",
    "X_test, y_test = get_model_data(filename = \"Test\")\n",
    "\n",
    "cat = [X_train.columns.get_loc(i) for i in [\"Team\",\"Div\",\"Opponent\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler on training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Scale training and test data\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "X_valid = scaler.transform(X_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "lazy_clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = lazy_clf.fit(X_train, X_valid, y_train, y_valid)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_lazy_report(X_test,y_test,lazy_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1966, number of negative: 3280\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001776 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7445\n",
      "[LightGBM] [Info] Number of data points in the train set: 5246, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374762 -> initscore=-0.511842\n",
      "[LightGBM] [Info] Start training from score -0.511842\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.68      0.92      0.78      1212\n",
      "        True       0.69      0.30      0.41       740\n",
      "\n",
      "    accuracy                           0.68      1952\n",
      "   macro avg       0.69      0.61      0.60      1952\n",
      "weighted avg       0.68      0.68      0.64      1952\n",
      "\n",
      "AUC 0.706\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "params = {'learning_rate': 0.01007397282357752, 'num_leaves': 12, 'max_depth': 3, 'min_child_samples': 6}\n",
    "\n",
    "# Create a LGBMClassifier model\n",
    "clf = LGBMClassifier(**params)  # Use **params to pass the dictionary as keyword arguments\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found by GridSearchCV\n",
    "\n",
    "# Call the custom_classification_report function\n",
    "custom_classification_report(X=X_test, y=y_test, model=clf)\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dump(clf, open(CONFIG.DATA_FOLDER_MODELS + 'lgb.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16,  0, 55, 41, 11, 11,  0, 89, 37,  2,  0,  1,  2,  0,  0, 39,  1,\n",
       "       15,  0,  0,  0,  0, 34,  0,  9, 11, 31,  0,  0, 33, 97,  4,  0,  0,\n",
       "        0,  0, 20, 22, 20, 31,  1,  9, 20,  5, 33])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.booster_.feature_importance(importance_type='split')  # 'split' or 'gain'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch für bestes Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B365_Team</td>\n",
       "      <td>0.468098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>B365_Team_odd_pred</td>\n",
       "      <td>0.142169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>BW_Team_odd_pred</td>\n",
       "      <td>0.077514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Avg_Opponent</td>\n",
       "      <td>0.047664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Avg_Team</td>\n",
       "      <td>0.039862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BW_Draw</td>\n",
       "      <td>0.036606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BW_Opponent</td>\n",
       "      <td>0.031168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Min_Draw</td>\n",
       "      <td>0.028843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Max_Team</td>\n",
       "      <td>0.025619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B365_Opponent</td>\n",
       "      <td>0.025413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              features  importance\n",
       "7            B365_Team    0.468098\n",
       "30  B365_Team_odd_pred    0.142169\n",
       "36    BW_Team_odd_pred    0.077514\n",
       "22        Avg_Opponent    0.047664\n",
       "17            Avg_Team    0.039862\n",
       "3              BW_Draw    0.036606\n",
       "2          BW_Opponent    0.031168\n",
       "26            Min_Draw    0.028843\n",
       "15            Max_Team    0.025619\n",
       "5        B365_Opponent    0.025413"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi = pd.DataFrame() \n",
    "fi['features'] = features\n",
    "fi['importance'] = clf.booster_.feature_importance(importance_type='gain')  \n",
    "fi['importance'] = fi['importance']/sum(fi['importance'])\n",
    "fi.sort_values(by='importance',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>B365_Team_odd_pred</td>\n",
       "      <td>0.138571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B365_Team</td>\n",
       "      <td>0.127143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BW_Opponent</td>\n",
       "      <td>0.078571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BW_Draw</td>\n",
       "      <td>0.058571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Max_Team</td>\n",
       "      <td>0.055714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IW_Opponent</td>\n",
       "      <td>0.052857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Avg_Opponent</td>\n",
       "      <td>0.048571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Ratio_Draw</td>\n",
       "      <td>0.047143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Span_Draw_last_4_games</td>\n",
       "      <td>0.047143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>B365_Team_last_4_games</td>\n",
       "      <td>0.044286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  features  importance\n",
       "30      B365_Team_odd_pred    0.138571\n",
       "7                B365_Team    0.127143\n",
       "2              BW_Opponent    0.078571\n",
       "3                  BW_Draw    0.058571\n",
       "15                Max_Team    0.055714\n",
       "8              IW_Opponent    0.052857\n",
       "22            Avg_Opponent    0.048571\n",
       "29              Ratio_Draw    0.047143\n",
       "44  Span_Draw_last_4_games    0.047143\n",
       "39  B365_Team_last_4_games    0.044286"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "params = {'learning_rate': 0.01007397282357752, 'num_leaves': 12, 'max_depth': 3, 'min_child_samples': 6}\n",
    "# Create a LGBMClassifier model\n",
    "clf = LGBMClassifier(params)\n",
    "\n",
    "# Create a GridSearchCV object to perform grid search\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found by GridSearchCV\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "custom_classification_report(X = X_test,y = y_test,model = grid_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Define an objective function to optimize\n",
    "def objective(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 20),\n",
    "    }\n",
    "\n",
    "    # Split your data into training and validation sets\n",
    "    # X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a LightGBM dataset\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "    # Specify the number of boosting rounds\n",
    "    num_boost_round = 10000\n",
    "\n",
    "    # Initialize variables for early stopping\n",
    "    early_stopping_rounds = 100\n",
    "    early_stopping_counter = 0\n",
    "    best_logloss = float('inf')\n",
    "\n",
    "    # Train the LightGBM model\n",
    "    clf = lgb.LGBMClassifier(**params, n_estimators=num_boost_round)\n",
    "    clf.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    val_pred = clf.predict_proba(X_valid, num_iteration=clf.best_iteration_)[:, 1]\n",
    "    logloss = log_loss(y_valid, val_pred)\n",
    "\n",
    "    if logloss < best_logloss:\n",
    "        best_logloss = logloss\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= early_stopping_rounds:\n",
    "            return best_logloss  # Early stopping\n",
    "\n",
    "    return best_logloss\n",
    "\n",
    "# Create a study object and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a LightGBM classifier with the best hyperparameters\n",
    "clf = lgb.LGBMClassifier(**best_params, n_estimators=100)  # You can specify a large number of estimators\n",
    "clf.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=100)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = clf.predict(X_valid)\n",
    "\n",
    "# Calculate accuracy on the validation set\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "print(\"Validation Accuracy:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "# Define pretrainer model architecture\n",
    "pretrainer = TabNetPretrainer(\n",
    "optimizer_fn=torch.optim.Adam,\n",
    "optimizer_params=dict(lr=2e-2),\n",
    "mask_type=\"entmax\"\n",
    ")\n",
    "\n",
    "# Train pretrainer model on training data\n",
    "pretrainer.fit(\n",
    "    X_train=X_train,\n",
    "    eval_set=[X_valid],\n",
    "    max_epochs=1000,\n",
    "    patience=30,\n",
    "    pretraining_ratio=0.8,\n",
    "    batch_size= 64\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "# Define hyperparameters\n",
    "n_d = 8\n",
    "n_a = 8\n",
    "n_steps = 3\n",
    "gamma = 1\n",
    "lambda_sparse = 0.001\n",
    "lr = 0.0001\n",
    "batch_size = 16\n",
    "max_epochs = 150\n",
    "\n",
    "# Create TabNet classifier\n",
    "clf = TabNetClassifier(n_d=n_d, n_a=n_a, cat_dims=cat,n_steps=n_steps, gamma=gamma, lambda_sparse=lambda_sparse, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=lr), mask_type='entmax', device_name='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Train TabNet classifier\n",
    "clf.fit(X_train=X_train, y_train=y_train,     eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'], eval_metric=['auc','balanced_accuracy'],batch_size=batch_size, max_epochs=max_epochs, patience=0)\n",
    "# from_unsupervised=pickle.load(open('tabnet.pkl', 'rb'))\n",
    "\n",
    "import pickle\n",
    "pickle.dump(clf, open('tabnet.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(clf, open('tabnet.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.load(open('tabnet.pkl', 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(clf.history['loss'], label='Validation Loss')\n",
    "plt.plot(clf.history['valid_auc'], label='AUC')\n",
    "plt.plot(clf.history['valid_balanced_accuracy'], label='Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "r = classification_report(y_pred=y_pred,y_true=y_test)\n",
    "print(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erklärbarkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_i = pd.Series(clf.feature_importances_)\n",
    "f_i.index = features\n",
    "f_i.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"BW_opponent_odd_pred\",\"B365_Opponent\",\"Avg_Opponent\",\"IW_Team\",\"Span_Draw\"      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(f_i>0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Hypertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, brier_score_loss\n",
    "\n",
    "def load_dataset():\n",
    "    # Load your dataset here\n",
    "    # Replace this with your dataset loading code\n",
    "    # X and y should be your feature and target variables\n",
    "    pass\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    shrink_threshold = trial.suggest_float(\"shrink_threshold\", 0.0, 1.0)\n",
    "    \n",
    "    X_train, y_train = get_model_data(filename = \"Train\")\n",
    "    X_valid, y_valid = get_model_data(filename = \"Valid\")\n",
    "    X_test, y_test = get_model_data(filename = \"Test\")\n",
    "    \n",
    "    # Create and train the NearestCentroid classifier with the suggested parameters\n",
    "    clf = NearestCentroid(shrink_threshold=shrink_threshold)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    \n",
    "    # Calculate the accuracy of the model\n",
    "    score = brier_score_loss(y_valid, y_pred)\n",
    "    \n",
    "    return brier_score_loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your dataset\n",
    "    \n",
    "    # Specify the SQLite database file for the study\n",
    "    study_name = \"neareast_centroid_optimization.db\"\n",
    "    study = optuna.create_study(direction=\"minimize\", study_name=study_name)\n",
    "\n",
    "    # Optimize the study\n",
    "    study.optimize(objective, n_trials=100)  # You can adjust the number of trials\n",
    "\n",
    "    # Get the best parameters and their corresponding accuracy\n",
    "    best_params = study.best_params\n",
    "    best_accuracy = study.best_value\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Accuracy:\", best_accuracy)\n",
    "\n",
    "    # # Save the study to the specified SQLite database file\n",
    "    # study.trials_dataframe().to_sql(study_name, \"sqlite:///{}\".format(study_name), if_exists=\"replace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    clf = NearestCentroid(shrink_threshold=0.95)\n",
    "    clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
